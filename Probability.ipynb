{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eebf783a-07fe-4d04-80b1-57b1a96a5980",
   "metadata": {},
   "source": [
    "First we are asked to plot $$\\sigma(x) = \\frac{1}{1+e^{-x}},$$\n",
    "$$ \\zeta(x) = \\log(1+e^x).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468c9646-2128-4e84-bcc7-632f4ea7c881",
   "metadata": {},
   "source": [
    "When $x$ is small $e^{-x} \\approx 1$, so $\\sigma(x)$ intersects the $y$ axis at $\\frac{1}{2}$. For large negative $x$, $e^{-x}$ is very large and for large positive $x$, $e^{-x}$ is very small. Hence $\\sigma(x) \\approx 0$ for large negative $x$ and $\\sigma(x) \\approx 1$ for large positive $x$.\n",
    "\n",
    "Similarly we can say that when $x$ is large and negative $\\zeta \\approx \\log(1) = 0$. When $x = 0$, we have a $y$-intercept of $\\log(2) \\approx 0.7$. As $x$ gets large, $1+e^{x} \\rightarrow e^x$. What I mean by this last statement is that out of $1$ and $e^x$, $e^x$ dominantes the expression. Hence $\\log(1+e^{x}) \\rightarrow x$.\n",
    "\n",
    "The graphs are given below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdd6bdf-5c20-46ae-9a3f-48640d979fcc",
   "metadata": {},
   "source": [
    "![alt text](Probability_Image1.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99836446-0cbc-40fb-85df-244b35a378bb",
   "metadata": {},
   "source": [
    "By multiplying $\\sigma(x)$ on the top and bottom, we obtain:\n",
    "$$\\sigma(x) = \\frac{e^x}{e^x + 1}.  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d789d8e-6b2f-4b66-8b3b-0b0fef2f4b98",
   "metadata": {},
   "source": [
    "Also, $$ \\frac{d \\sigma(x)}{dx}  = -1 \\times (1+e^{-x} )^{-2} \\times -1 \\times e^{-x} = \\frac{e^{-x}}{(1+e^{-x})^2} = \\frac{1+e^{-x} - 1}{(1+e^{-x})^2} = \\frac{1}{1+e^{-x}}\\bigg(1- \\frac{1}{1+e^{-x}} \\bigg) = \\sigma(x)(1-\\sigma(x)). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5fe125-e821-46d1-8a77-43a8d8444936",
   "metadata": {},
   "source": [
    "The next result is very simple: $$ \\sigma(x) + \\sigma(-x) = \\frac{1}{1+e^{-x}} + \\frac{1}{1+e^{x}} = \\frac{e^x + 2 + e^{-x}}{(1+e^x)(1+e^{-x})} = 1.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8c383b-416d-4cbe-a297-67b28f935927",
   "metadata": {},
   "source": [
    "Next, $$ \\log \\sigma(x) = \\log \\frac{1}{1+e^{-x}} = \\log(1) - \\log(1+e^{-x}) =  - \\log(1+e^{-x}) = -\\zeta(-x).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615aeb8b-3221-41ce-b69d-8243059dd019",
   "metadata": {},
   "source": [
    "The next two indentites require us to find inverses:\n",
    "\n",
    "Let $$y = \\frac{1}{1+e^{-x}} \\implies \\frac{1}{y}  = 1+ e^{-x} \\implies \\frac{1 - y}{y} = e^{-x} \\implies \\log\\bigg(\\frac{y}{1-y}\\bigg) = x.$$\n",
    "\n",
    "Hence: $$ \\sigma^{-1}(x) = \\log\\bigg(\\frac{x}{1-x}\\bigg). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec5c3d8-b95b-43c2-9ac9-109d2e3dc05d",
   "metadata": {},
   "source": [
    "For the other inverse: $$ y = \\log(1+e^x)  \\implies e^y = 1 + e^x \\implies \\log(e^y -1) = x.$$\n",
    "Hence:\n",
    "$$ \\zeta^{-1}(x) = \\log(e^x -1). $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cac198-b164-4c88-aa1c-d63499ae31e3",
   "metadata": {},
   "source": [
    "Now for some integration, $$ \\int_{-\\infty}^{x} \\sigma(y) dy = \\int_{-\\infty}^{x} \\frac{1}{1+e^{-y}} dy.$$\n",
    "If we let $z=e^{-y}$, then $\\frac{dz}{dy} = -e^{-y} = -z.$ Making the substitution, we obtain:\n",
    "\n",
    "$$ \\int_{-\\infty}^{x} \\frac{1}{1+e^{-y}} dy = \\int_{e^{-x}}^{\\infty} \\frac{1}{z(z+1)} dz = \\int_{e^{-x}}^{\\infty} \\frac{1}{z} - \\frac{1}{z+1} dz = \\bigg[\\log z - \\log(z+1)\\bigg]_{e^{-x}}^{\\infty} = \\bigg[\\log\\bigg(\\frac{z}{z+1}\\bigg)\\bigg]_{e^{-x}}^{\\infty} = \\log\\bigg(\\frac{e^{-x} + 1}{e^{-x}}\\bigg) = \\log(1+e^x) = \\zeta(x).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5692acf7-3813-4090-b127-35503c829b4b",
   "metadata": {},
   "source": [
    "Finally: $$ \\zeta(x) - \\zeta(-x) = \\log(1+e^x) - \\log(1+e^{-x}) = \\log \\bigg(\\frac{1+e^x}{1+e^{-x}}\\bigg) = \\log \\bigg(\\frac{e^{x}(1+e^{-x})}{1+e^{-x}}\\bigg) = \\log(e^x) = x,$$ as required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03412a82-4ecc-450a-880c-86401ada6292",
   "metadata": {},
   "source": [
    "We now need to prove some results around probability mass functions. The first is quite easy:\n",
    "$$P(0) + P(1) = 1-\\phi + \\phi = 1.$$\n",
    "For the expectation and variance, let $X$ be a random variable with the probability mass function $P$:\n",
    "$$ E[X] = \\sum_{x = 0}^{1} x P(x) = \\phi,$$\n",
    "$$ Var(X) = E(X^2) - [E(X)]^2 = \\phi - \\phi^2 = \\phi(1-\\phi).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcda30b-e29f-4354-b01f-23852a80b1b4",
   "metadata": {},
   "source": [
    "For the second mass function: $$ \\sum_{k=0}^{n} {n \\choose k} \\phi^k (1-\\phi)^{n-k} = (\\phi + (1-\\phi))^n = 1.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d51fa9-e347-41aa-8ced-20845ee201c4",
   "metadata": {},
   "source": [
    "Since $$E(X)  = \\sum_{k=0}^{n} k{n \\choose k} \\phi^k (1-\\phi)^{n-k},$$\n",
    "and $$\\sum_{k=0}^{n} {n \\choose k} \\phi^k (1-\\phi)^{n-k} = 1,$$\n",
    "we can differentiate with respect to $\\phi$ to obtain:\n",
    "$$ 0 = \\frac{1}{\\phi}E(X) + \\frac{1}{1-\\phi}E(X) - \\frac{n}{1-\\phi} \\implies E(X) = n\\phi.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965e67e7-526d-4618-ac7b-c23fb08f26c9",
   "metadata": {},
   "source": [
    "To obtain the variance, we differentiate $ \\sum_{k=0}^{n} {n \\choose k} \\phi^k (1-\\phi)^{n-k} = 1$ twice to obtain an expression involving $E(X^2)$. After a lot of algebra we arrive at the following expression:\n",
    "$$ E(X^2)[(1-\\phi)^2 + 2\\phi(1-\\phi) + \\phi^2] + E(X)[-(1-\\phi)^2 - 2n\\phi(1-\\phi) + (1-2n)\\phi^2] + n(n-1)\\phi^2 = 0,$$\n",
    "which simplifies to $$ E(X^2) = n \\phi + \\phi^2n^2 - \\phi^2n .$$\n",
    "Therefore,\n",
    "$$ Var(X) =  E(X^2) - [E(X)]^2 = n \\phi(1-\\phi).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf8cab1-6ac2-46c1-8a04-5dce905ae689",
   "metadata": {},
   "source": [
    "We could have avoided large swathes of algebra by noting that $P(k) = \\sum_{k=0}^{n} {n \\choose k} \\phi^k (1-\\phi)^{n-k} $ is the probability mass function of a variable that is binomially distributed. A random variable that is distributed binomially is the sum of $n$ independent Bernoulli variables. Since we know that for a Bernoulli random variable the mean is $\\phi$ and the variance is $\\phi(1-\\phi)$ (computed above), we can use the following formulas:\n",
    "$$ E(X + Y) = E(X) + E(Y) \\text{ and } Var(X+Y) = Var(X) + Var(Y).$$\n",
    "The result follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55660f3f-36b4-46bf-ae11-0b6c073d891f",
   "metadata": {},
   "source": [
    "For the third mass function we have $$P(k) = \\phi(1- \\phi)^{k-1}, k = 1,2,...$$\n",
    "It is trivial to show that it is valid:\n",
    "$$\\sum_{k = 1}^{\\infty} \\phi(1- \\phi)^{k-1}=  \\phi \\frac{1}{\\phi} = 1.$$\n",
    "Now for the expectation:\n",
    "$$ E(X) = \\sum_{k = 1}^{\\infty} k \\phi (1-\\phi)^{k-1}.$$\n",
    "All we need to do is differentiate the following expression:\n",
    "$$ \\sum_{k = 1}^{\\infty} (1-\\phi)^k = \\frac{1}{\\phi} - 1 \\implies \\sum_{k = 1}^{\\infty} k(1-\\phi)^{k-1} = \\frac{1}{\\phi^2}.$$\n",
    "Multiplying through by $\\phi$ gives us the result. That is, $E(X) = \\frac{1}{\\phi}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031ff52e-79a7-4bae-be57-3dbe07e86eeb",
   "metadata": {},
   "source": [
    "We need to find $E(X^2)$ to calculate $Var(X)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e51d220-afeb-4974-97bd-1e262e4baec0",
   "metadata": {},
   "source": [
    "![alt text](Probability_Image2.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cd8036-11c0-4689-b63f-df6323ff1a41",
   "metadata": {},
   "source": [
    "Hence $$Var(X) = \\frac{1-\\phi}{\\phi^2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dbf2a6-389c-4e55-bac6-0be7124a44ce",
   "metadata": {},
   "source": [
    "For the final probability mass function:\n",
    "$$ P(k) = e^{-\\lambda}\\frac{\\lambda^{n}}{k!}.$$\n",
    "I will answer this with my reMarkable:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65954a67-3293-4c7c-8fb6-16106aba9919",
   "metadata": {},
   "source": [
    "![alt text](Probability_Image3.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52984a1-e941-4ff3-8c7a-c11153abbf59",
   "metadata": {},
   "source": [
    "Now for the probability density functions, $p(x)$. First up is the uniform distribution: $$p(x) = \\frac{1}{b-a}.$$\n",
    "\n",
    "validity (trivial):\n",
    "$$\\int^b_a  \\frac{1}{b-a} dx = 1.$$\n",
    "Expectation (let $X$ be a random variable with pmf...):\n",
    "$$E[X] = \\int^b_a  \\frac{x}{b-a} dx = \\frac{1}{b-a} \\bigg[\\frac{x^2}{2}\\bigg] = \\frac{a+b}{2}.$$\n",
    "Also,\n",
    "$$ E[X^2] = \\frac{a^2 + ab + b^2}{3},$$\n",
    "hence with some basic algebra:\n",
    "$$Var(X) = \\frac{(b-a)^2}{12}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ac286d-208a-44f2-abca-219dbb471783",
   "metadata": {},
   "source": [
    "For the probability density function $$p(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{\\big(\\frac{x-\\mu}{\\sqrt{2}\\sigma}\\big)^2}.$$ \n",
    "All the working on my reMarkable:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e76ad8-b062-4538-bdc7-25d8d0a4165f",
   "metadata": {},
   "source": [
    "![alt text](Probability_Image4.png \"Title\")\n",
    "![alt text](Probability_Image5.png \"Title\")\n",
    "![alt text](Probability_Image6.png \"Title\")\n",
    "![alt text](Probability_Image7.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925afab5-9620-44e8-8eba-884be20b499f",
   "metadata": {},
   "source": [
    "The final probability density function is quite easy to work with. It is $$ p(x) = \\lambda e^{-\\lambda x}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5825d419-f1a8-4492-8d64-1d39c9bc8b34",
   "metadata": {},
   "source": [
    "To show that it is valid:\n",
    "$$ \\int \\lambda e^{-\\lambda x} dx = \\big[-e^{-\\lambda x}\\big]_0^{\\infty} = 1. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8bede7-14ab-4c4a-8e1e-b042756c75e5",
   "metadata": {},
   "source": [
    "For the expectation, all we need to use is integration by parts. Let $X$ be a random variable that is distributed according to $p$:\n",
    "$$E(X) = \\int_0^{\\infty}(x \\lambda)e^{-\\lambda x} dx = \\frac{1}{\\lambda} \\int_0^{\\infty}ze^{-z} dz = - \\frac{1}{\\lambda} \\big[z e^{-z} \\big]_0^{\\infty} +  \\frac{1}{\\lambda} \\int_0^{\\infty}e^{-z} dz= \\frac{1}{\\lambda}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c4bf45-9895-4d3e-a63b-a123baa9320d",
   "metadata": {},
   "source": [
    "For the varaince, we need $E(X^2)$. We apply integration by parts in the same way we did above. We obtain $E(X^2) = \\frac{2}{\\lambda^2}.$ Hence $$ Var(X) = \\frac{1}{\\lambda^2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabfa4b7-7309-420c-8bd2-f24fba14e62e",
   "metadata": {},
   "source": [
    "# Spam Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4debaac1-331a-4778-9b26-ab044775f329",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

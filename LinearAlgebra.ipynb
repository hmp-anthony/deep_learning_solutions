{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f92e2707-8aae-4dfd-8315-d2ab8458456c",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4714823c-5411-40d2-9e9d-2ab9903d7ba9",
   "metadata": {},
   "source": [
    "Prove that $\\textbf{x}\\textbf{B}\\textbf{x} = 0$ if $\\textbf{B}$ is the anti-symmetric part of a matrix $\\textbf{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa3ee4a-a4be-4f71-be79-4ac08c4e678f",
   "metadata": {},
   "source": [
    "$$\\textbf{x}^T(\\textbf{A} - \\textbf{A}^T)\\textbf{x} = \\textbf{x}^T\\textbf{A}\\textbf{x} - \\textbf{x}^T\\textbf{A}^T\\textbf{x} = \\textbf{x}^T(\\textbf{A}\\textbf{x}) - (\\textbf{A}\\textbf{x})^T\\textbf{x} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b23268b-503b-46ce-8da4-5f3f1193edec",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d482cd1c-7c61-4292-b89f-95e7dfa5adc7",
   "metadata": {},
   "source": [
    "What is the gradient of $tr(\\mathbf{A})$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3436e3-1ef7-45f5-ad30-bff11fbf985b",
   "metadata": {},
   "source": [
    "$$\\lim_{\\delta \\mathbf{A} \\to 0} \\frac{tr(\\mathbf{A} + \\delta \\mathbf{A} ) - tr(\\mathbf{A})}{\\delta \\mathbf{A}} = \\lim_{\\delta \\mathbf{A} \\to 0} \\frac{tr(\\delta \\mathbf{A})}{\\delta \\mathbf{A}} = \\lim_{\\delta \\mathbf{A} \\to 0} \\bigg[ \\frac{\\sum_{i}\\delta A_{ii}}{\\delta A_{ij}}\\bigg]_{i,j} = \\mathbb{I}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5039f4d1-846b-434d-8339-38543c814144",
   "metadata": {},
   "source": [
    "A less hand-waving solution is: $$\\frac{\\partial tr(\\mathbf{A})}{\\partial A_{ij}} = \\bigg[ \\frac{\\Sigma_k A_{kk}}{\\partial A_{ij}} \\bigg]_{ij}= \\delta_{ij}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7550bfc5-a35a-4bcc-9767-3532f9c3daeb",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4399b14b-2c80-4df1-acd6-87dd37aad7a2",
   "metadata": {},
   "source": [
    "Find the gradient of the sigmoid function:\n",
    "$$ \\sigma(\\mathbf{x}) = \\frac{1}{1 + \\text{exp}(-\\mathbf{w}^T\\mathbf{x}) } $$\n",
    "Let us look at the partial derivatives:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7c9d12-866a-49a0-b185-88f2ff4ff1f6",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial \\sigma}{\\partial x_i} = -1(1 + \\text{exp}(-\\Sigma_i w_ix_i))^{-2} \\times \\frac{\\partial}{\\partial x_i}(1 + \\text{exp}(-\\Sigma_i w_ix_i)) $$\n",
    "$$= -1(1 + \\text{exp}(-\\Sigma_i w_ix_i))^{-2} \\times \\frac{\\partial}{\\partial x_i}(\\text{exp}(-\\Sigma_i w_ix_i)) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e2933b-28b2-45b0-8b5d-d257de31a947",
   "metadata": {},
   "source": [
    "We need the chain rule: If $ h(x) = f(g(x))$, then $h'(x) = f'(g(x))g'(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc7f50e-9edb-459e-955f-9168e539cc66",
   "metadata": {},
   "source": [
    "$f(x) = e^x \\implies f'(x) = e^x$, and $g(\\textbf{x}) = -\\Sigma_i w_ix_i \\implies \\frac{\\partial g}{\\partial x_i} = -w_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e756f31e-e13c-49bd-875b-1e744b05d2de",
   "metadata": {},
   "source": [
    "Hence $$ \\frac{\\partial \\sigma}{\\partial x_i} = w_i \\frac{\\text{exp}(-\\Sigma_i w_ix_i)}{(1+\\text{exp}(-\\Sigma_i w_ix_i))^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f9f18c-bb9c-4a66-bf3d-8399f6ae4b3f",
   "metadata": {},
   "source": [
    "Now if we let $e =  \\text{exp}(-\\mathbf{w}^T\\mathbf{x})$, \n",
    "$$ \\frac{\\partial \\sigma}{\\partial x_i} = w_i \\frac{e}{(1+e)^2} = w_i \\frac{e+1 -1}{(1+e)^2} = w_i\\bigg[ \\frac{1}{1+e} - \\frac{1}{(1+e)^2} \\bigg] = w_i\\sigma(1-\\sigma)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc771a46-9280-4cbf-83c4-8218c2790d73",
   "metadata": {},
   "source": [
    "# Q 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e810313-f65f-4863-8a7c-ede184a94ba8",
   "metadata": {},
   "source": [
    "What is $$ \\nabla_{\\mathbf{x}} \\frac{\\mathbf{x}^T\\mathbf{A}\\mathbf{x}}{||\\mathbf{x}||_2^2} ?$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5d9412-2632-4a87-b7ab-7370fd991fbe",
   "metadata": {},
   "source": [
    "We can use $$\\nabla \\frac{f}{g} = \\frac{g \\nabla f - f \\nabla g}{g^2},$$ where $f(\\mathbf{x}) = \\mathbf{x}^T\\mathbf{A}\\mathbf{x}$, and $g(\\mathbf{x}) = ||\\mathbf{x}||^2_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2b92b4-9107-4c4b-a3c9-8d07b25207b9",
   "metadata": {},
   "source": [
    "Since $g(\\mathbf{x}) = \\Sigma_i x_i^2$, $\\frac{\\partial g}{\\partial x_k} = 2x_k$. Hence $$\\nabla g = 2\\mathbf{x}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3ec08e-ce7b-4e12-ae63-2ddd96b1649f",
   "metadata": {},
   "source": [
    "Similarly, $$f(\\mathbf{x}) = \\sum_i \\sum_j x_iA_{ij}x_j = \\sum_{i \\neq k} \\sum_{j \\neq k} x_iA_{ij}x_j + \\sum_j x_kA_{kj}x_j + \\sum_i x_iA_{ik}x_k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d0b693-3d7e-4a23-9bc6-9b7d53ade8f9",
   "metadata": {},
   "source": [
    "Therefore: $$ \\frac{\\partial f(\\mathbf{x})}{\\partial x_k} = (\\mathbf{A}\\mathbf{x})_k + (\\mathbf{A}^T\\mathbf{x})_k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf037687-26b0-428e-bace-30f15339b1c5",
   "metadata": {},
   "source": [
    "Or $$ \\nabla f(\\mathbf{x}) = (\\mathbf{A} + \\mathbf{A}^T)\\mathbf{x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8022a5d-5a69-4c81-a02b-fa4362bfb331",
   "metadata": {},
   "source": [
    "So, if $\\mathbf{A}$ is symmetric, we obtain $$ \\nabla_{\\mathbf{x}} \\frac{\\mathbf{x}^T\\mathbf{A}\\mathbf{x}}{||\\mathbf{x}||_2^2}  = 2\\frac{(\\textbf{x}^T\\textbf{x}) \\mathbf{A}\\mathbf{x} - (\\mathbf{x}^T\\mathbf{A}\\mathbf{x}) \\mathbf{x}}{||x||_2^4}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6e4af9-145d-4d49-9a2f-c094add4bea5",
   "metadata": {},
   "source": [
    "This makes perfect sense. If $\\mathbf{A}$ was the identity matrix then we would obtain a zero gradient. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9fc6a9-2f01-402e-a165-3394b237cf51",
   "metadata": {},
   "source": [
    "# Q5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05355f5-1050-4fa6-b9fe-125bd25af9ae",
   "metadata": {},
   "source": [
    "Is $\\mathbf{A}^T\\mathbf{A}$ positive definite, symmetric and/or positive semi-definite?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1028c796-7ded-4f31-92c2-c0d5ba85c786",
   "metadata": {},
   "source": [
    "Well,  $(\\mathbf{A}^T\\mathbf{A})^T = \\mathbf{A}^T\\mathbf{A}$, and $\\mathbf{x}^T\\mathbf{A}^T\\mathbf{A}\\mathbf{x} = (\\mathbf{A}\\mathbf{x})^T\\mathbf{A}\\mathbf{x} = ||\\mathbf{A}\\mathbf{x}||^2 \\geq 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fe3b3f-a900-4c26-942b-2ebbb5a2c2a5",
   "metadata": {},
   "source": [
    "That is, the matrix is symmetric and positive semi-definite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375e782c-aab4-4565-9d04-532346d8730a",
   "metadata": {},
   "source": [
    "# Q 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721b96a9-e7d3-4ac9-bff9-0e6bd658dc0a",
   "metadata": {},
   "source": [
    "Prove that: $$ tr(\\mathbf{ABC}) = tr(\\mathbf{CAB}) .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f3b651-b315-425a-94b8-d17dde34dd87",
   "metadata": {},
   "source": [
    "To prove this we will first show that $ tr(\\mathbf{AB}) = tr(\\mathbf{BA}):$\n",
    "$$tr(\\mathbf{AB})  = \\sum_k (\\mathbf{AB})_{kk} = \\sum_{k}\\sum_{j} A_{kj}B_{jk} \\sum_{j}\\sum_{k}B_{jk} A_{kj} = \\sum_j (\\mathbf{BA})_{jj} = tr(\\mathbf{BA}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65305ac9-5584-4e67-941c-96c80511bc02",
   "metadata": {},
   "source": [
    "Hence $$tr(\\mathbf{ABC}) = tr((\\mathbf{AB})\\mathbf{C}) = tr(\\mathbf{C}(\\mathbf{AB})) = tr(\\mathbf{CAB}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e70065-2c35-4d71-8f8f-7be53bc4de31",
   "metadata": {},
   "source": [
    "# Q 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7903f1e9-4491-4d03-ad5b-954150004475",
   "metadata": {},
   "source": [
    "Show that if $\\mathbf{A}$ is symmetric then $det(\\mathbf{A}) = \\prod_i \\lambda_i$ and $tr(\\mathbf{A}) = \\sum_i \\lambda_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cb3fd4-4577-48d0-bf00-037726f5f7e0",
   "metadata": {},
   "source": [
    "If $\\mathbf{A}$ is symmetric then we can write $\\mathbf{A} = \\mathbf{\\Sigma}\\mathbf{D}\\mathbf{\\Sigma}^T$, where $\\mathbf{\\Sigma}$ is orthogonal and $\\mathbf{D}$ is a diagonal matrix of eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a6bdef-be73-4219-ae19-3b27634861e9",
   "metadata": {},
   "source": [
    "Therefore: \n",
    "$$ det(\\mathbf{A}) = det(\\mathbf{\\Sigma})det(\\mathbf{D})det(\\mathbf{\\Sigma}^T) = det(\\mathbf{D}) = \\prod_i \\lambda_i.$$\n",
    "\n",
    "Also:\n",
    "$$ tr(\\mathbf{A}) = tr(\\mathbf{\\Sigma}\\mathbf{D}\\mathbf{\\Sigma}^T) = tr(\\mathbf{\\Sigma^T}\\mathbf{\\Sigma}\\mathbf{D}) = tr(\\mathbf{D}) = \\sum_i \\lambda_i.$$\n",
    "where we have used $\\det(\\mathbf{AB}) = \\det(\\mathbf{A})\\det(\\mathbf{B})$, and question 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d2023a-92d7-436f-b44a-9c650d31998a",
   "metadata": {},
   "source": [
    "# Q 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a193de38-5a4b-419c-b677-157f3a53637a",
   "metadata": {},
   "source": [
    "(a) is easy, let $$   \\mathbf{A}=\n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   1 & 0 \\\\\n",
    "   0 & 0 \\\\\n",
    "  \\end{array} } \\right].$$\n",
    "  \n",
    "  $\\mathbf{A}$ is diagonalizable (trivially) and is singular.\n",
    "\n",
    "\n",
    "(b) Suppose \n",
    "$$   \\mathbf{M}=\n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   1 & 1 \\\\\n",
    "   0 & 1 \\\\\n",
    "  \\end{array} } \\right]$$\n",
    "\n",
    "$\\mathbf{M}$ is clearly non-singular and it has a repeated eigenvalue $\\lambda = 1$. My justification for it not being diagonalizable is that \n",
    "\n",
    "$$  (\\mathbf{M} - \\lambda \\mathbf{I}) \\mathbf{v}=\n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   0 & 1 \\\\\n",
    "   0 & 0 \\\\\n",
    "  \\end{array} } \\right] \\mathbf{v} = 0 \\implies v_2 = 0.$$\n",
    "\n",
    "That is, the eigenspace is one-dimensional, so we cannot form a matrix of linearly independent eigenvectors $\\mathbf{V}$ that span the eigenspace.  More generally, we can say:\n",
    "\n",
    "$\\textit{ A matrix is diagonalizable if and only if the algebraic multiplicity equals the geometric multiplicity of each of the eigenvalues.}$\n",
    "\n",
    "In our example the algebraic multiplicity of the eigenvalue 1 is 2 and the geometric multiplicity of the eigenvalue 1 is 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f3e7d0-3ec0-475d-8a1f-79f3e798107e",
   "metadata": {},
   "source": [
    "# Q 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27abb0d3-1dc2-46bc-9d6b-71e289e7a8b0",
   "metadata": {},
   "source": [
    "The matrix chosen in this question does not really illustrate the point that the exercise means to get across. The eigenvalues are not simple values, which leads to a mess. Instead consider the following matrix \n",
    "$$  \\mathbf{A} = \\frac{1}{2}\n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   5 & 1 \\\\\n",
    "   1 & 5 \\\\\n",
    "  \\end{array} } \\right] .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4c3987-657e-4ee5-b76b-fb9a62c67feb",
   "metadata": {},
   "source": [
    "This matrix has eigenvlaues $\\lambda_1 = 3$ and $\\lambda_2 = 2$ and eigenvectors $\\mathbf{v}_1 = (1,1)$ and $\\mathbf{v}_2 = (1,-1)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9cd0ba-5317-4b39-856e-b60788dd927f",
   "metadata": {},
   "source": [
    "The idea here is that the matrix scales eigenvectors. So if we choose \"points\" on the unit circle corresponding to eigenvectors, we can see that they are mapped to scalar multiples of themselves. That is, a circle is mapped to an ellipse. In the direction of $\\mathbf{v}_1$ the circle is stretched by a factor of 3 and in the direction of $\\mathbf{v}_2$ the circle is stretched by a factor of 2. So we have mapped a circle centered at the origin with radius 1 to an ellipse centered at the origin that has semi-major axis 3 and semi-minor axis 2 (and rotated by 45 degrees)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7811dc5c-bc70-4f95-9309-12adfdf4cba6",
   "metadata": {},
   "source": [
    "# Q 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e0c9be-befe-4a62-b6a9-32d86884cb7d",
   "metadata": {},
   "source": [
    "What is the SVD of $$  \\mathbf{A} = \n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   3 & 2 \\\\\n",
    "   2 & 4 \\\\\n",
    "   1 & 2 \\\\\n",
    "  \\end{array} } \\right] ?$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7327e210-742f-4c7f-894e-43839f14c036",
   "metadata": {},
   "source": [
    "FFS. Another messy question with no real educational content. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c6b983-b905-4fd6-890a-f76e2e15f508",
   "metadata": {},
   "source": [
    "# Programming Assignment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad03ee7-1a41-445f-bcc7-fc9316115da9",
   "metadata": {},
   "source": [
    "I chose to do this assignment in R. For a regression task, especially using non-linear transformations, R is by-far easier to work with than Python. The work involved in this question does not match the knowledge gained, especially if you choose to use Python: this is typical with this handbook so far. See Q 9 for an example of that. The idea behind this question is to show that we can increase the complexity of the model (by using increasing integer powers of the predictor), and this complexity is coupled with a decrease in the training error. However, this increase in complexity is not coupled by a decrease in the testing error.\n",
    "\n",
    "The difference between optimisation and machine learning is that in optimisation we only care about minimising a function on existing data, but in machine learning we care how our model works on data that the model has not \"seen\". This is clearly demonstrated on a task like this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
